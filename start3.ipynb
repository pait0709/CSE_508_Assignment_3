{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Google News vectors\n",
    "model= KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "dog = model['dog']\n",
    "print(dog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize(sentence):\n",
    "    words=sentence\n",
    "    words_vecs = [model[word] for word in words if word in model]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(300)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocessed.pkl','rb')as f:\n",
    "    preprocessed=pickle.load(f)\n",
    "\n",
    "with open('ratings.pkl','rb')as f:\n",
    "    ratings=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in preprocessed:\n",
    "    x.append(vectorize(i))\n",
    "\n",
    "y=[]\n",
    "for i in ratings:\n",
    "    if i==3:\n",
    "        y.append(2)\n",
    "    elif i>3:\n",
    "        y.append(3)\n",
    "    elif i<3:\n",
    "        y.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480402959231859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.40      0.41      1258\n",
      "           2       0.16      0.17      0.16       651\n",
      "           3       0.79      0.79      0.79      4444\n",
      "\n",
      "    accuracy                           0.65      6353\n",
      "   macro avg       0.45      0.45      0.45      6353\n",
      "weighted avg       0.65      0.65      0.65      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report\n",
    "DT=DecisionTreeClassifier()\n",
    "DT.fit(x_train,y_train)\n",
    "y_pred=DT.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.54      0.61      1258\n",
      "           2       0.36      0.03      0.06       651\n",
      "           3       0.80      0.97      0.88      4444\n",
      "\n",
      "    accuracy                           0.78      6353\n",
      "   macro avg       0.63      0.51      0.52      6353\n",
      "weighted avg       0.74      0.78      0.74      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "model = LogisticRegression(max_iter=1000,multi_class=\"ovr\")\n",
    "\n",
    "model=OneVsOneClassifier(model)\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.54      0.61      1258\n",
      "           2       0.36      0.03      0.06       651\n",
      "           3       0.80      0.97      0.88      4444\n",
      "\n",
      "    accuracy                           0.78      6353\n",
      "   macro avg       0.63      0.51      0.52      6353\n",
      "weighted avg       0.74      0.78      0.74      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "model=OneVsOneClassifier(model)\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.37      0.50      1258\n",
      "           2       0.77      0.04      0.07       651\n",
      "           3       0.76      0.98      0.86      4444\n",
      "\n",
      "    accuracy                           0.76      6353\n",
      "   macro avg       0.77      0.46      0.48      6353\n",
      "weighted avg       0.77      0.76      0.71      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.59      0.65      1258\n",
      "           2       0.33      0.00      0.01       651\n",
      "           3       0.81      0.97      0.88      4444\n",
      "\n",
      "    accuracy                           0.80      6353\n",
      "   macro avg       0.62      0.52      0.51      6353\n",
      "weighted avg       0.74      0.80      0.75      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model=SVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pait/anaconda3/envs/gensim_masti/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.53      0.61      1258\n",
      "           2       0.30      0.02      0.04       651\n",
      "           3       0.80      0.97      0.88      4444\n",
      "\n",
      "    accuracy                           0.78      6353\n",
      "   macro avg       0.60      0.51      0.51      6353\n",
      "weighted avg       0.73      0.78      0.74      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model=LinearSVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.44      0.51      1258\n",
      "           2       0.23      0.08      0.11       651\n",
      "           3       0.79      0.93      0.85      4444\n",
      "\n",
      "    accuracy                           0.74      6353\n",
      "   macro avg       0.54      0.48      0.49      6353\n",
      "weighted avg       0.70      0.74      0.71      6353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('filtered_df.pkl','rb')as f:\n",
    "    filtered_df=pickle.load(f)\n",
    "unique_users={}\n",
    "unique_items={}\n",
    "user_num=0\n",
    "item_num=0\n",
    "for x in range(len(filtered_df)):\n",
    "    if filtered_df.iloc[x]['reviewerID'] not in unique_users:\n",
    "        unique_users[filtered_df.iloc[x]['reviewerID']]=user_num\n",
    "        user_num=user_num+1\n",
    "    if filtered_df.iloc[x]['asin'] not in unique_items:\n",
    "        unique_items[filtered_df.iloc[x]['asin']]=item_num\n",
    "        item_num=item_num+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22542\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "print(user_num)\n",
    "print(item_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B0001TST6W': 0, 'B00083Y87U': 1, 'B000KRGI4K': 2, 'B0017KZEI0': 3, 'B0036WHNVS': 4, 'B00443HVJW': 5, 'B004DTU4DM': 6, 'B004UVC3HS': 7, 'B004XKA9ZO': 8, 'B00501M426': 9, 'B00512Z8H2': 10, 'B0052Z83KC': 11, 'B0056BIYQA': 12, 'B0058LOZMA': 13, 'B005GQBVWO': 14, 'B005K22JHA': 15, 'B005Z294OQ': 16, 'B0076LTKTS': 17, 'B007D29QA8': 18, 'B008M324PG': 19, 'B0092Z479Y': 20, 'B00953THVQ': 21, 'B009CKU5JU': 22, 'B009SYM9T4': 23, 'B00A7I5U9G': 24, 'B00AAVEXZW': 25, 'B00AIIMSTA': 26, 'B00AIIMSRW': 27, 'B00AKDEPFI': 28, 'B00AZO3TZO': 29, 'B00B6KR8S0': 30, 'B00B8BF4EM': 31, 'B00BETSOEU': 32, 'B00BGO0Q9O': 33, 'B00BKEQBI0': 34, 'B00BLA5PYY': 35, 'B00CABCL9K': 36, 'B00CIV693Q': 37, 'B00CRGO6AK': 38, 'B00CRKJ2AU': 39, 'B00D7LN7PO': 40, 'B00D8JIPJI': 41, 'B00DHBDDX0': 42, 'B00DLJQ7S6': 43, 'B00DM2APM6': 44, 'B00DOJCMBO': 45, 'B00DOTG38W': 46, 'B00DY03N2U': 47, 'B00E1QXN7Q': 48, 'B00EDDAMT4': 49, 'B00EDDC58A': 50, 'B00EJ3FJGE': 51, 'B00F4BXJ58': 52, 'B00FI16HS0': 53, 'B00FI16820': 54, 'B00FQMZMTQ': 55, 'B00FRG98I2': 56, 'B00FRG98JG': 57, 'B00FTROUXM': 58, 'B00FZHUGVQ': 59, 'B00G5DAVSM': 60, 'B00GGGQHPO': 61, 'B00GJUR7CY': 62, 'B00GSAR1QW': 63, 'B00GT6B92W': 64, 'B00GTBH4Q2': 65, 'B00H0ONT20': 66, 'B00HBNKY78': 67, 'B00HFRDBNO': 68, 'B00HIDCE6U': 69, 'B00HIDLH54': 70, 'B00HMKP1S2': 71, 'B00HNDBL8M': 72, 'B00HNU7YFY': 73, 'B00HQIN4DY': 74, 'B00HQIFKWW': 75, 'B00HS32XQQ': 76, 'B00HSXSJD2': 77, 'B00IAIFF4A': 78, 'B00ID7CQOK': 79, 'B00IGIEAAO': 80, 'B00IMC8ENS': 81, 'B00IOOB1RU': 82, 'B00IREZHD6': 83, 'B00IRM05K8': 84, 'B00J3BAR28': 85, 'B00J4DY8RU': 86, 'B00J59OEEK': 87, 'B00J6FK942': 88, 'B00JGSODKU': 89, 'B00JKDQ0KW': 90, 'B00JL9AY52': 91, 'B00JL9AY02': 92, 'B00JP09KE8': 93, 'B00JP05ZUG': 94, 'B00JQ8MCOY': 95, 'B00JQ8MCBC': 96, 'B00JQ8MCFI': 97, 'B00JQTOZY8': 98, 'B00JSO9UD2': 99, 'B00JTT2FBU': 100, 'B00JU3XHC6': 101, 'B00JZM7ZVQ': 102, 'B00K3EPV52': 103, 'B00K6BJZEK': 104, 'B00KA7TL5S': 105, 'B00KASUMD2': 106, 'B00KGGK6Y8': 107, 'B00KHX4GCI': 108, 'B00KJ322V8': 109, 'B00KLFS4GG': 110, 'B00KN3NN3A': 111, 'B00KWW3VVQ': 112, 'B00L1G4MG0': 113, 'B00L3LDUU2': 114, 'B00L61STNW': 115, 'B00L67XUVM': 116, 'B00L8CVUFI': 117, 'B00L8D0BAM': 118, 'B00LAFOU6E': 119, 'B00LAXAO0W': 120, 'B00LB2ZQ3C': 121, 'B00LD40O7Q': 122, 'B00LF0CLKQ': 123, 'B00LFHJVFC': 124, 'B00LGJ8Y5W': 125, 'B00LHLOAUM': 126, 'B00LHSAOW8': 127, 'B00LL3RLLG': 128, 'B00LL5T64E': 129, 'B00LQI28C8': 130, 'B00LUHT1MK': 131, 'B00LV0DPE6': 132, 'B00LWLMGSU': 133, 'B00M2UV9JW': 134, 'B00M34PVYQ': 135, 'B00M35Q0F4': 136, 'B00M7CBFNA': 137, 'B00MBFYUGM': 138, 'B00MLSYROO': 139, 'B00MQ99YB4': 140, 'B00MUF1548': 141, 'B00MUO8HBI': 142, 'B00MV96GO2': 143, 'B00N0NJEF6': 144, 'B00N1Z5BHI': 145, 'B00N2M9KK4': 146, 'B00N6U8DSC': 147, 'B00N9OAQI0': 148, 'B00NAL6CP8': 149, 'B00NC8PMUK': 150, 'B00NCQ9FD2': 151, 'B00NG0QJDI': 152, 'B00NI0RMKK': 153, 'B00NJF2ZFG': 154, 'B00NU6J3JA': 155, 'B00NWRPAKS': 156, 'B00NXOPMHQ': 157, 'B00NYPPASK': 158, 'B00NZ9CRJU': 159, 'B00O63TMGU': 160, 'B00O66AP4A': 161, 'B00O95BP1A': 162, 'B00OAOFP6G': 163, 'B00OB50U7I': 164, 'B00ODOX2PO': 165, 'B00OHGT6VW': 166, 'B00OMSD52Q': 167, 'B00OOQV1KY': 168, 'B00OPLDY3U': 169, 'B00OPUEWD2': 170, 'B00OPYT3TQ': 171, 'B00OQ13SPI': 172, 'B00OR7YJSQ': 173, 'B00OW4Z01Y': 174, 'B00OXE2UNE': 175, 'B00OXE9636': 176, 'B00P0UM6NO': 177, 'B00P2VA1OW': 178, 'B00P2V9YFY': 179, 'B00P7JAW1Q': 180, 'B00P8P93R8': 181, 'B00P8YO3DS': 182, 'B00PGMP3AY': 183, 'B00PGMP3EK': 184, 'B00PGMP3CC': 185, 'B00PGX4DWW': 186, 'B00PI7IGHE': 187, 'B00PQD206I': 188, 'B00PQD1Y56': 189, 'B00PR40CHA': 190, 'B00PRWKMCM': 191, 'B00PRX70IK': 192, 'B00PUZ6QTO': 193, 'B00Q1T3Y2K': 194, 'B00Q1T3236': 195, 'B00Q5VBWK0': 196, 'B00Q799MWU': 197, 'B00Q7HHA12': 198, 'B00Q8KDU6C': 199, 'B00Q8V023U': 200, 'B00Q9S9RJC': 201, 'B00QAHT3LE': 202, 'B00QGQQCOK': 203, 'B00QMXRRV4': 204, 'B00QUOPPLY': 205, 'B00QUWJ5EO': 206, 'B00R10HX2A': 207, 'B00R1UVDWG': 208, 'B00R3TG2P8': 209, 'B00R5W5VKA': 210, 'B00RHPPOI4': 211, 'B00RHUDQFC': 212, 'B00RTWJUGW': 213, 'B00RTWJRTM': 214, 'B00RTWKBPG': 215, 'B00RWBJGZA': 216, 'B00RY1Z9NQ': 217, 'B00S39ASMK': 218, 'B00S7MY7QQ': 219, 'B00S7MY7NE': 220, 'B00S7MY7LQ': 221, 'B00S7MY7NY': 222, 'B00S95LIN6': 223, 'B00S9QXU0O': 224, 'B00SAHG71Q': 225, 'B00SAI6Z62': 226, 'B00SF1AUY2': 227, 'B00SGZ47Z0': 228, 'B00SHM80HI': 229, 'B00SKO90TU': 230, 'B00SKOIBMW': 231, 'B00SKO90QS': 232, 'B00SR9WGBM': 233, 'B00SV07BX0': 234, 'B00T4S455A': 235, 'B00T9RGVAI': 236, 'B00T9RNSMC': 237, 'B00TCVHGZU': 238, 'B00TG13D62': 239, 'B00TR1PRLK': 240, 'B00TUKWGLM': 241, 'B00U2C9HMS': 242, 'B00U6TRMII': 243, 'B00U8IX7FO': 244, 'B00UJC03A6': 245, 'B00UKYBCIA': 246, 'B00UOPHHMK': 247, 'B00UOUCA9A': 248, 'B00UR7ERGE': 249, 'B00UXZQF38': 250, 'B00V48Y9T0': 251, 'B00V62R2RA': 252, 'B00V6336HO': 253, 'B00VF0IXEY': 254, 'B00VF5NT4I': 255, 'B00VGPO38S': 256, 'B00VGQFQXI': 257, 'B00VKSI8RS': 258, 'B00VMB5XH6': 259, 'B00VMPVQDC': 260, 'B00VO0QBSK': 261, 'B00VOW6UFC': 262, 'B00VT50P6O': 263, 'B00VV7QNNE': 264, 'B00W3E5FYW': 265, 'B00W3IKGUQ': 266, 'B00W74QGQO': 267, 'B00W997I6Y': 268, 'B00WECGIXK': 269, 'B00WKMM4NM': 270, 'B00WKMQTVA': 271, 'B00WM1K910': 272, 'B00WNAX246': 273, 'B00WNT2680': 274, 'B00WP9NFUG': 275, 'B00WRHW1K6': 276, 'B00WTOOJB6': 277, 'B00WUKULAC': 278, 'B00WVJWDYO': 279, 'B00WVMBB0S': 280, 'B00X5SNXDU': 281, 'B00X65514G': 282, 'B00X65515K': 283, 'B00X6550VK': 284, 'B00X9L7S52': 285, 'B00X9L8C3E': 286, 'B00X9L90RG': 287, 'B00XBQ93Q2': 288, 'B00XC2FA8K': 289, 'B00XC2FAM6': 290, 'B00XC2FA1C': 291, 'B00XJE4A3C': 292, 'B00XKLE1RO': 293, 'B00XLYHRUS': 294, 'B00XN3MLYY': 295, 'B00XTDGJ3W': 296, 'B00XW34AMQ': 297, 'B00XXALLSE': 298, 'B00Y83G32E': 299, 'B00Y83URXU': 300, 'B00YCT51EA': 301, 'B00YGJN6UC': 302, 'B00YH2H316': 303, 'B00YKNNKXW': 304, 'B00YKNY612': 305, 'B00YKNW8GC': 306, 'B00YL0U58W': 307, 'B00YTP4FHQ': 308, 'B00YTXEPR8': 309, 'B00YTZFWB4': 310, 'B00YTZM68Q': 311, 'B00YUDPFFS': 312, 'B00YUDR0VA': 313, 'B00ZABBRZQ': 314, 'B00ZCFQ71E': 315, 'B00ZFLZ3F6': 316, 'B00ZFNNWZM': 317, 'B00ZFY7YAK': 318, 'B00ZHPMUDS': 319, 'B00ZN07GBS': 320, 'B00ZNQE4JE': 321, 'B00ZZPTZBA': 322, 'B01024X0Y6': 323, 'B0106IS5XY': 324, 'B010NOJ10W': 325, 'B010OX42WO': 326, 'B010T2I3HK': 327, 'B0111XWC7I': 328, 'B011729DWK': 329, 'B011718DQS': 330, 'B011BNGA8A': 331, 'B011BRK4EM': 332, 'B011EVQBG0': 333, 'B011HT9HOM': 334, 'B011HT9AD0': 335, 'B011HT99RW': 336, 'B011HT9AEY': 337, 'B011IFQDTW': 338, 'B011KRTC82': 339, 'B011KRTGM4': 340, 'B011KRT872': 341, 'B011KYAVWG': 342, 'B011LOXTYM': 343, 'B011N8S04K': 344, 'B011RK7ZT0': 345, 'B011RK807Q': 346, 'B011TNI8YG': 347, 'B011TYHRBA': 348, 'B011VB1X6Q': 349, 'B0126UC3D8': 350, 'B012C6S36W': 351, 'B012DMX5BI': 352, 'B012E0M5C4': 353, 'B012HYF4A2': 354, 'B012J47WZA': 355, 'B012VQE8LM': 356, 'B0131Z993E': 357, 'B013G4VIJ8': 358, 'B013GCH2GS': 359, 'B013GDRSMK': 360, 'B013H2NGOO': 361, 'B013HW8ZH2': 362, 'B013LKLIC4': 363, 'B013LKLS2E': 364, 'B013Q2Y28O': 365, 'B013QGO9ZQ': 366, 'B013WF3Y52': 367, 'B013YDFHGQ': 368, 'B013ZWYZTG': 369, 'B013ZWYZP0': 370, 'B014009O2A': 371, 'B0140J1XCK': 372, 'B014275A4M': 373, 'B0142A96G2': 374, 'B0142XSQ38': 375, 'B014F0QZ54': 376, 'B014H56KG6': 377, 'B014INJ0VO': 378, 'B014IX9NK2': 379, 'B014M8YPCO': 380, 'B014R47QJC': 381, 'B014ZTDC88': 382, 'B0151JOANM': 383, 'B0153I8NRU': 384, 'B01576WAQS': 385, 'B0157LQTX8': 386, 'B0158CCLFG': 387, 'B0158CCPBQ': 388, 'B0158CCLM4': 389, 'B015C5SHO8': 390, 'B015CHYPMY': 391, 'B015DYK74G': 392, 'B015E2IH7Q': 393, 'B015EFHHTW': 394, 'B015H2EYL6': 395, 'B015H2E8NK': 396, 'B015HHRNL4': 397, 'B015HI1CAQ': 398, 'B015JQ60NA': 399, 'B015LOOY38': 400, 'B015MHLKWM': 401, 'B015N8XRHG': 402, 'B015NUQYTC': 403, 'B015NUQYQA': 404, 'B015PCBBI2': 405, 'B015PZGJ72': 406, 'B015U6MO7A': 407, 'B015UZ8DJO': 408, 'B015XP3JEU': 409, 'B015ZE1UQS': 410, 'B015ZE1V8U': 411, 'B0163CHND0': 412, 'B0163HRGSW': 413, 'B01649SM4G': 414, 'B0169KO73U': 415, 'B0169MXB7Q': 416, 'B016A3SBYM': 417, 'B016BM0T2E': 418, 'B016CKGXMU': 419, 'B016K3KWQC': 420, 'B016KMX6AW': 421, 'B016MQ1FRC': 422, 'B016OKV7XS': 423, 'B016VUVAHE': 424, 'B016XBL3F0': 425, 'B016XFCJ4U': 426, 'B016XFCG9I': 427, 'B0171MB9QS': 428, 'B0175U4YN6': 429, 'B01762UQDK': 430, 'B0176BWEYU': 431, 'B0176DX8IE': 432, 'B0177BB7EC': 433, 'B0177LLVZM': 434, 'B0178W37P2': 435, 'B0179C568C': 436, 'B0179C56G4': 437, 'B017DCZ8IQ': 438, 'B017H7U008': 439, 'B017IHAXYK': 440, 'B017IM9K26': 441, 'B017IRQJPM': 442, 'B017ITXNLI': 443, 'B017J31L80': 444, 'B017JNGLPI': 445, 'B017PWAWIA': 446, 'B0181TNKRQ': 447, 'B0183UT6LM': 448, 'B0183UT43W': 449, 'B0183UT1K8': 450, 'B0184O3S3E': 451, 'B018AE2CV2': 452, 'B018E9CZHY': 453, 'B018E9CX9E': 454, 'B018GDDECI': 455, 'B018JMMW9W': 456, 'B018LC5ZGC': 457, 'B018NIZHO4': 458, 'B018R0CAZM': 459, 'B018SB95FS': 460, 'B018TEOCNE': 461, 'B018TQSW4C': 462, 'B018WWJJGI': 463, 'B01982PJMY': 464, 'B019AXIYF0': 465, 'B019CFXI2A': 466, 'B019F61HRE': 467, 'B019FGOSZW': 468, 'B019FSWYYW': 469, 'B019LJS03Y': 470, 'B019N3SD4O': 471, 'B019REINNU': 472, 'B019REINVM': 473, 'B019VM3GKI': 474, 'B019YOFUF2': 475, 'B019YW26SI': 476, 'B019YW27DW': 477, 'B019Z44B74': 478, 'B01A0PRQSI': 479, 'B01A41XG5U': 480, 'B01A6PVW9G': 481, 'B01A6Q9L4I': 482, 'B01A9INDS8': 483, 'B01AAOWBMA': 484, 'B01ABEJF5A': 485, 'B01AJD0SXQ': 486, 'B01AJOYX54': 487, 'B01AJOZ0JC': 488, 'B01AJOXS3C': 489, 'B01AJP0EUG': 490, 'B01AJOZXNA': 491, 'B01AJOYYOY': 492, 'B01AJOXTYK': 493, 'B01AJP0JM4': 494, 'B01AK9UDG6': 495, 'B01AN2OS7K': 496, 'B01AN385VO': 497, 'B01AS0V2I0': 498, 'B01ASZ2XT2': 499, 'B01AT2T8YW': 500, 'B01AT3GCIG': 501, 'B01AZ22QLS': 502, 'B01B18Y4Z6': 503, 'B01B25PX30': 504, 'B01B4J4WCW': 505, 'B01B5XH2FQ': 506, 'B01B5XFJGU': 507, 'B01B6P9XF0': 508, 'B01B7FVNM0': 509, 'B01BD0J2H2': 510, 'B01BDX9Q5C': 511, 'B01BKUB6BA': 512, 'B01BLQT7DM': 513, 'B01BWW9XRU': 514, 'B01C05E9PE': 515, 'B01C0EQWFK': 516, 'B01C2LFMXO': 517, 'B01C639LFS': 518, 'B01C8HLEGQ': 519, 'B01CBGV3T2': 520, 'B01CHY15QO': 521, 'B01CP335MY': 522, 'B01CPMHZGW': 523, 'B01CQD1EF8': 524, 'B01CQIUHT2': 525, 'B01CQOGE9S': 526, 'B01CS95C4E': 527, 'B01CTY8PUG': 528, 'B01CUV2ZA4': 529, 'B01CY1QXR6': 530, 'B01CYDIMVE': 531, 'B01CYRFH04': 532, 'B01CYRSFUS': 533, 'B01D0YMRXK': 534, 'B01D17EG3A': 535, 'B01D1H6MDW': 536, 'B01D1WY8KQ': 537, 'B01D2PNBS2': 538, 'B01D2TQG62': 539, 'B01D467VT4': 540, 'B01D4FTRHO': 541, 'B01D75IJ16': 542, 'B01D84OBR2': 543, 'B01D8GJ32I': 544, 'B01DBAUX52': 545, 'B01DCC4206': 546, 'B01DCFVP6M': 547, 'B01DCFVPTE': 548, 'B01DD99OQA': 549, 'B01DD99H9Y': 550, 'B01DDUHJJS': 551, 'B01DDWO3TK': 552, 'B01DDWO3MC': 553, 'B01DDWO3MM': 554, 'B01DDWO3YK': 555, 'B01DEQSC4W': 556, 'B01DEW1H4I': 557, 'B01DF2H732': 558, 'B01DG44K0C': 559, 'B01DKERPG4': 560, 'B01DM5AJUU': 561, 'B01DMX1SEI': 562, 'B01DNTWGYM': 563, 'B01DPF5XH6': 564, 'B01DQQP1L2': 565, 'B01DUE7EHO': 566, 'B01DVTXLKM': 567, 'B01DVXZR6Y': 568, 'B01DXHVFS2': 569, 'B01DXKMULA': 570, 'B01DXRE5H0': 571, 'B01DY2PL2W': 572, 'B01DY35M52': 573, 'B01DZR3Z5G': 574, 'B01E01IUEW': 575, 'B01E01IUBK': 576, 'B01E0V5PHW': 577, 'B01E1GI618': 578, 'B01E37ADUM': 579, 'B01E3CMKP8': 580, 'B01E4S6Y4E': 581, 'B01E5GAACW': 582, 'B01E5QI6GO': 583, 'B01E70CUW4': 584, 'B01E9T8TFU': 585, 'B01EB297MO': 586, 'B01ECGV8DA': 587, 'B01EGQU5CQ': 588, 'B01EHS5FQE': 589, 'B01EHRDFSU': 590, 'B01EHSLX2E': 591, 'B01EHSMJLS': 592, 'B01EHTOD3E': 593, 'B01EJ7SOVQ': 594, 'B01EKUH6X4': 595, 'B01EMU7W04': 596, 'B01EMU7Z4M': 597, 'B01ENG02XG': 598, 'B01EQAP9AK': 599, 'B01EQVI7G2': 600, 'B01EQWI6XA': 601, 'B01EQWQSQC': 602, 'B01ER3PWLC': 603, 'B01ERGOGKM': 604, 'B01ESSDBL4': 605, 'B01EUDPIMM': 606, 'B01EX5PU5W': 607, 'B01EY2XM7C': 608, 'B01EYR0BUS': 609, 'B01F26SSIC': 610, 'B01F2F8AUO': 611, 'B01F30FTN4': 612, 'B01F3AGJL0': 613, 'B01F4O0NKI': 614, 'B01F7Q2RCK': 615, 'B01F7TOB0S': 616, 'B01F81UPE6': 617, 'B01F8RNMYA': 618, 'B01FA0LZJE': 619, 'B01FCTMTHK': 620, 'B01FD9ZHGY': 621, 'B01FDK0D7G': 622, 'B01FDPW1NK': 623, 'B01FENVYJ8': 624, 'B01FGF371C': 625, 'B01FH20RUI': 626, 'B01FH98RXU': 627, 'B01FHMVU04': 628, 'B01FICM7PA': 629, 'B01FIGJ66O': 630, 'B01FJH8WV2': 631, 'B01FJOWH3E': 632, 'B01FKEA758': 633, 'B01FL7KWYK': 634, 'B01FLXACTE': 635, 'B01FQLGCHM': 636, 'B01FQO29T4': 637, 'B01FQO0SDI': 638, 'B01FRSHXUO': 639, 'B01FRSHYZ8': 640, 'B01FRSHXUY': 641, 'B01FSBYA2E': 642, 'B01FSFA1PU': 643, 'B01FTZF7V2': 644, 'B01FVMAD1C': 645, 'B01FVVH06Y': 646, 'B01FX10OX8': 647, 'B01FX74I7K': 648, 'B01FXGVXJM': 649, 'B01G07SVO8': 650, 'B01G13AGKS': 651, 'B01G1TBJZ8': 652, 'B01G3J9VKG': 653, 'B01G46AF0S': 654, 'B01G7HYF3W': 655, 'B01G7ZPSM6': 656, 'B01G9ZBILE': 657, 'B01GBFCQ0E': 658, 'B01GE0IE3E': 659, 'B01GE6N01E': 660, 'B01GFJIIS0': 661, 'B01GG4SEB0': 662, 'B01GGLF48E': 663, 'B01GGL3XXM': 664, 'B01GH22R96': 665, 'B01GJQF1Q6': 666, 'B01GJZP1IA': 667, 'B01GM9404E': 668, 'B01GUO65MQ': 669, 'B01H3I9QUG': 670, 'B01H59DRJ4': 671, 'B01H5YX9HO': 672, 'B01H6G7ULC': 673, 'B01H734GSY': 674, 'B01H9VJETU': 675, 'B01HB9NWPC': 676, 'B01HB9D022': 677, 'B01HBE0AR0': 678, 'B01HCGEYTM': 679, 'B01HCH1LB0': 680, 'B01HCH2C0E': 681, 'B01HCH293E': 682, 'B01HCT3GCU': 683, 'B01HCT3JI6': 684, 'B01HCT3EEA': 685, 'B01HCT3LC0': 686, 'B01HE90L4Y': 687, 'B01HE966RK': 688, 'B01HHE09PM': 689, 'B004GAO1OQ': 690, 'B00N2M9KX6': 691, 'B00O8MBALO': 692, 'B00T5W631Y': 693, 'B00WGA5N86': 694, 'B00Y4I9J2O': 695, 'B0109ZLA9A': 696, 'B011BKYVX4': 697, 'B015AB6DNQ': 698, 'B0171NKAR6': 699, 'B019NGVT4M': 700, 'B01ABC5GYQ': 701, 'B01BB1PJGG': 702, 'B01CFV6B8Q': 703, 'B01CFV6DZC': 704, 'B01DDXKUW8': 705, 'B01DTZIZF4': 706, 'B01FDEJKN0': 707, 'B01HD40II4': 708}\n"
     ]
    }
   ],
   "source": [
    "print(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_rating_matrix=np.zeros((22542,709))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(filtered_df)):\n",
    "    user_item_rating_matrix[unique_users[filtered_df.iloc[x]['reviewerID']],unique_items[filtered_df.iloc[x]['asin']]]=filtered_df.iloc[x]['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(user_item_rating_matrix[909])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_rating_matrix = (user_item_rating_matrix - user_item_rating_matrix.min()) / (user_item_rating_matrix.max() - user_item_rating_matrix.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.8 0.  0.  ... 0.  0.  0. ]\n",
      " [1.  0.  0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(user_item_rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_item_rating_matrix.pkl','wb') as f:\n",
    "    pickle.dump(user_item_rating_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 21262 is out of bounds for axis 0 with size 18033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Plotting MAE vs N\u001b[39;00m\n\u001b[1;32m     78\u001b[0m n_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m50\u001b[39m]\n\u001b[0;32m---> 79\u001b[0m user_user_maes \u001b[38;5;241m=\u001b[39m \u001b[43muser_user_recommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_item_rating_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m item_item_maes \u001b[38;5;241m=\u001b[39m item_item_recommender(user_item_rating_matrix, n_values)\n\u001b[1;32m     82\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[142], line 48\u001b[0m, in \u001b[0;36muser_user_recommender\u001b[0;34m(rating_matrix, n_values)\u001b[0m\n\u001b[1;32m     46\u001b[0m val_matrix \u001b[38;5;241m=\u001b[39m rating_matrix[val_index]\n\u001b[1;32m     47\u001b[0m top_n_indices \u001b[38;5;241m=\u001b[39m find_top_n_similar(user_similarity_matrix[train_index, :], n)\n\u001b[0;32m---> 48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_missing_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m mae \u001b[38;5;241m=\u001b[39m compute_mae(val_matrix, predictions)\n\u001b[1;32m     50\u001b[0m fold_maes\u001b[38;5;241m.\u001b[39mappend(mae)\n",
      "Cell \u001b[0;32mIn[142], line 23\u001b[0m, in \u001b[0;36mpredict_missing_ratings\u001b[0;34m(rating_matrix, top_n_indices, n)\u001b[0m\n\u001b[1;32m     21\u001b[0m user_ratings \u001b[38;5;241m=\u001b[39m rating_matrix[user_idx, :]\n\u001b[1;32m     22\u001b[0m top_n_user_indices \u001b[38;5;241m=\u001b[39m top_n_indices[user_idx]\n\u001b[0;32m---> 23\u001b[0m similar_users_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mrating_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_n_user_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m similar_users_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(similar_users_ratings \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m predictions[user_idx, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(similar_users_ratings \u001b[38;5;241m/\u001b[39m similar_users_weights[:, np\u001b[38;5;241m.\u001b[39mnewaxis], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m n\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21262 is out of bounds for axis 0 with size 18033"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume user_item_rating_matrix is the numpy matrix representing the user-item-rating data\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Function to find top N similar users/items\n",
    "def find_top_n_similar(similarity_matrix, n):\n",
    "    top_n_indices = np.argsort(-similarity_matrix, axis=1)[:, :n]\n",
    "    return top_n_indices\n",
    "\n",
    "# Function to predict missing ratings using top N similar users/items\n",
    "def predict_missing_ratings(rating_matrix, top_n_indices, n):\n",
    "    predictions = np.zeros_like(rating_matrix)\n",
    "    for user_idx in range(rating_matrix.shape[0]):\n",
    "        user_ratings = rating_matrix[user_idx, :]\n",
    "        top_n_user_indices = top_n_indices[user_idx]\n",
    "        similar_users_ratings = rating_matrix[top_n_user_indices, :]\n",
    "        similar_users_weights = np.sum(similar_users_ratings != 0, axis=1)\n",
    "        predictions[user_idx, :] = np.sum(similar_users_ratings / similar_users_weights[:, np.newaxis], axis=0) / n\n",
    "    return predictions\n",
    "\n",
    "# Function to compute Mean Absolute Error (MAE)\n",
    "def compute_mae(actual_ratings, predicted_ratings):\n",
    "    diff = np.abs(actual_ratings - predicted_ratings)\n",
    "    return np.mean(diff[actual_ratings != 0])\n",
    "\n",
    "# User-User Recommender System\n",
    "def user_user_recommender(rating_matrix, n_values):\n",
    "    user_similarity_matrix = np.zeros((rating_matrix.shape[0], rating_matrix.shape[0]))\n",
    "    for i in range(rating_matrix.shape[0]):\n",
    "        for j in range(i + 1, rating_matrix.shape[0]):\n",
    "            user_similarity_matrix[i, j] = user_similarity_matrix[j, i] = cosine_similarity(rating_matrix[i, :], rating_matrix[j, :])\n",
    "\n",
    "    maes = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for n in n_values:\n",
    "        fold_maes = []\n",
    "        for train_index, val_index in kf.split(rating_matrix):\n",
    "            train_matrix = rating_matrix[train_index]\n",
    "            val_matrix = rating_matrix[val_index]\n",
    "            top_n_indices = find_top_n_similar(user_similarity_matrix[train_index, :], n)\n",
    "            predictions = predict_missing_ratings(train_matrix, top_n_indices, n)\n",
    "            mae = compute_mae(val_matrix, predictions)\n",
    "            fold_maes.append(mae)\n",
    "        maes.append(np.mean(fold_maes))\n",
    "\n",
    "    return maes\n",
    "\n",
    "# Item-Item Recommender System\n",
    "def item_item_recommender(rating_matrix, n_values):\n",
    "    item_similarity_matrix = np.zeros((rating_matrix.shape[1], rating_matrix.shape[1]))\n",
    "    for i in range(rating_matrix.shape[1]):\n",
    "        for j in range(i + 1, rating_matrix.shape[1]):\n",
    "            item_similarity_matrix[i, j] = item_similarity_matrix[j, i] = cosine_similarity(rating_matrix[:, i], rating_matrix[:, j])\n",
    "\n",
    "    maes = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for n in n_values:\n",
    "        fold_maes = []\n",
    "        for train_index, val_index in kf.split(rating_matrix):\n",
    "            train_matrix = rating_matrix[train_index]\n",
    "            val_matrix = rating_matrix[val_index]\n",
    "            top_n_indices = find_top_n_similar(item_similarity_matrix, n)\n",
    "            predictions = predict_missing_ratings(train_matrix.T, top_n_indices, n).T\n",
    "            mae = compute_mae(val_matrix, predictions)\n",
    "            fold_maes.append(mae)\n",
    "        maes.append(np.mean(fold_maes))\n",
    "\n",
    "    return maes\n",
    "\n",
    "# Plotting MAE vs N\n",
    "n_values = [10, 20, 30, 40, 50]\n",
    "user_user_maes = user_user_recommender(user_item_rating_matrix, n_values)\n",
    "item_item_maes = item_item_recommender(user_item_rating_matrix, n_values)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_values, user_user_maes, label='User-User Recommender')\n",
    "plt.plot(n_values, item_item_maes, label='Item-Item Recommender')\n",
    "plt.xlabel('N (Number of Similar Users/Items)')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('MAE vs N')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Report TOP 10 products by User Sum Ratings\n",
    "user_sum_ratings = np.sum(user_item_rating_matrix, axis=0)\n",
    "top_10_products = np.argsort(-user_sum_ratings)[:10]\n",
    "print(\"TOP 10 products by User Sum Ratings:\")\n",
    "print(top_10_products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
